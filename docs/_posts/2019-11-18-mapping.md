---
layout: post
title: "Mapping"
categories: notes
---

OpenCV includes many functions for calibrating and mapping points and images between different spaces.

### Homography

A homography is a transformation that reorients one image to match or fit inside another: 

* The two images are usually two points of view of the same subject.
* Common features are found in both images and paired to make a data set.
<div style="width:600px;margin:0 auto;" markdown="1">
[![Feature Matching + Homography to find Objects](https://docs.opencv.org/3.0-beta/_images/homography_findobj.jpg){:width="100%" text-align:"center"}
*Feature Matching + Homography to find Objects*](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html)
</div>
* The data set is then used with the [`cv::findHomography`](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography) function, which returns a matrix that allows us to warp one image into the other.
* We can then use this homography in [cv::warpPerspective`](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective), which takes in an image and *warps* it to match the other's perspective.

<div style="width:600px;margin:0 auto;" markdown="1">
[![Homography Examples using OpenCV](https://www.learnopencv.com/wp-content/uploads/2016/01/homography-example.jpg){:width="100%" text-align:"center"}![Homography Examples using OpenCV](https://www.learnopencv.com/wp-content/uploads/2016/01/homography-alignment.jpg){:width="100%" text-align:"center"}
*Homography Examples using OpenCV*](https://www.learnopencv.com/homography-examples-using-opencv-python-c/)
</div>

This operation can have many uses, like perspective correction, embedding images into one another, or combining many to create large panoramas. The feature pairs can be manually or automatically selected, using OpenCV feature detectors. 

<div style="width:600px;margin:0 auto;" markdown="1">
<iframe width="600" height="338" src="https://www.youtube.com/embed/GH1p1HtNegY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
*video panorama stitching with stabilizing homography estimation*
</div>

Let's use homography to map our bouncing ball sketch to a projection surface. 

We'll start with a couple of changes:

* We will add a second window for the projector in `main.cpp`.
* We will render our bouncing balls canvas into an `ofFbo`, and draw that FBO out to both our main and projector windows. 
* The FBO will be the size of the projection screen. We will use two macros `PROJECTOR_RESOLUTION_X` and `PROJECTOR_RESOLUTION_Y` to refer to these values across the app.
* We will interact using the mouse on the main screen, so we will have to remap our values in `mouseDragged()`. We only want to add balls when the mouse is dragging on top of the canvas, so we will check if the cursor is in the right spot using [`ofInRange()`](https://openframeworks.cc/documentation/math/ofMath/#!show_ofInRange).

{% gist 53124f25130a9ffe336560ca21398cba main.cpp %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-balls.h %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-balls.cpp %}
{% gist 53124f25130a9ffe336560ca21398cba ezBall.h %}
{% gist 53124f25130a9ffe336560ca21398cba ezBall.cpp %}

Next, let's add an interface for mapping feature points from one image to the next.

* The points will be normalized (between 0 and 1) to make them independent of image position or resolution. This will allow us to draw them properly in both windows with minimal headaches.
* The source points will never change, we'll just use the four corners of our image.
* The destination points can be changed by dragging them with the mouse. We'll write a simple system where the point nearest to the mouse gets selected on click.
* The destination points will also be drawn in the projection window, so that we can use them to against actual physical features we are projecting on.

{% gist 53124f25130a9ffe336560ca21398cba ofApp-points.h %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-points.cpp %}

Finally, we will use these points to first get a homography transformation, then use this transformation to warp our image on the fly.

Note that this might run slowly unless you're running your app in Release mode. This is because our projection FBO is quite large (1920x1080) and we need to read back from the GPU every frame to warp the image on the CPU.

{% gist 53124f25130a9ffe336560ca21398cba ofApp-homography.h %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-homography.cpp %}

### World to Projector Mapping

Homography is useful for 2D mapping (i.e. images to images), but we can also attempt to map the 3D space we are projecting onto back into the projector image.

<div style="width:600px;margin:0 auto;" markdown="1">
<iframe width="600" height="338" src="https://www.youtube.com/embed/CE1B7tdGCw0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
*UCLA's Augmented Reality Sandbox*
</div>

The idea is similar:

* We will collect pairs of corresponding points from both spaces. However, since one space is our world, we will need its points in 3D. We will use a depth sensor for this.
* We can then pass this data set to a solver function, which will give us a transformation we can apply to other points in the same space, and have them *project* to an appropriate position on screen.

#### Model View Projection

We've already been doing something similar when rendering 3D objects on screen, like point clouds. You may have heard of the Model View Projection Matrix (or MVP) when working with computer graphics.

* The Model View Projection is actually a stack of three matrices that are used to transform a 3D point from its local space to screen space.
* The *model matrix* maps a point from its local space to the world space.
* The *view matrix* maps a point from world space to camera space (from the point of view of the camera).
* The *projection matrix* maps a point from camera space to clip space, which is essentially what the camera projects onto a surface. Depending on the camera parameters, this projection can have perspective or be orthographic.

<div style="width:600px;margin:0 auto;" markdown="1">
[![Geometric Photo Manipulation - Projections](https://www.maa.org/sites/default/files/images/cms_upload/figure256786.jpg){:width="100%" text-align:"center"}
*Geometric Photo Manipulation - Projections*](https://www.maa.org/press/periodicals/loci/joma/geometric-photo-manipulation-projections)
</div>

What we are essentially doing is coming up with a similar matrix, but with an external camera and projector.

[Model View Projection](https://jsantell.com/model-view-projection) is a good reference for in-depth info.

#### Pinhole Camera Model

We use a pinhole camera model to calculate this projection. Using a perspective model, the line of sight from the camera to a 3D point will intersect a plane, which we can consider our canvas. The position at which it intersects the plane is its projection in 2D space.

<div style="width:600px;margin:0 auto;" markdown="1">
[![Camera Calibration and 3D Reconstruction](https://docs.opencv.org/2.4/_images/pinhole_camera_model.png){:width="100%" text-align:"center"}
*Camera Calibration and 3D Reconstruction*](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html)
</div>

#### ofxKinectProjectorToolkit

[ofxKinectProjectorToolkit](https://github.com/genekogan/ofxKinectProjectorToolkit) is one of the many addons available for OF to create a correspondence between the 3D world and a 2D projector.

* This addon was originally written by Gene Kogan but it's a little out of date. I've made a fork [here](https://github.com/prisonerjohn/ofxKinectProjectorToolkit/tree/feature/refactor-0.10) with updates which should get you started faster.
* As the name implies, this addon is meant to work with the Kinect sensor. However, the calibration functions are sensor-agnostic. You just need to provide point pairs and it does the rest.
* A chessboard pattern is used for feature detection. The chessboard is commonly used in OpenCV because it is high contrast and easy to track. The [`cv::findChessboardCorners()`](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findchessboardcorners) function is used to track the intersection points in the pattern.
* The chessboard is drawn out of the projector. Because we are drawing the points, we know their 2D position on screen, in projector space.
* The chessboard is detected using the depth sensor's color camera, which is looking at the projection surface. The tracked points are then fed to the sensor's world coordinate mapper to extract corresponding 3D world points.
* The two sets of points are then fed to a solver, which determines the transformation matrix from one space to the other. 
* `ofxKinectProjectorToolkit` uses [dlib](http://dlib.net/) for calibration, which is another commonly used image processing library.
* If we were to use OpenCV, we would probably use the [`cv::calibrateCamera()`](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#calibratecamera) function.

#### Projected Blob Clipping

The following is UNTESTED code that uses this addon to attempt to segment out a blob from a projected image. We'll try it in class and see what happens :)

Here is the version for Kinect.

{% gist 53124f25130a9ffe336560ca21398cba ofApp-kinect.h %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-kinect.cpp %}

And the version for RealSense.

{% gist 53124f25130a9ffe336560ca21398cba ofApp-realsense.h %}
{% gist 53124f25130a9ffe336560ca21398cba ofApp-realsense.cpp %}
